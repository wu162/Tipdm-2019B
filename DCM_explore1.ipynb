{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCM_explore1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wu162/Tipdm-2019B/blob/master/DCM_explore1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "FDwqEHm5f_e0",
        "colab_type": "code",
        "outputId": "88bc407c-2259-419c-b95b-35d18907f346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install dicom"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dicom in /usr/local/lib/python3.6/dist-packages (0.9.9.post1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7XmJAZeriBbL",
        "colab_type": "code",
        "outputId": "61186af0-f5c0-4654-cce7-00b25b5573e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.9 GB  | Proc size: 117.5 MB\n",
            "GPU RAM Free: 15079MB | Used: 0MB | Util   0% | Total 15079MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kT4VpS8fi8lP",
        "colab_type": "code",
        "outputId": "f41177ae-6a3c-40cc-e8c1-bf61aef60296",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131304 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dCUDOtPLi9NO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 指定Google Drive云端硬盘的根目录，名为drive\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qBDU7Jhijx9j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 指定当前的工作文件夹\n",
        "import os\n",
        "\n",
        "# 此处为google drive中的文件路径,drive为之前指定的工作根目录，要加上\n",
        "os.chdir(\"drive/Colab Notebooks/\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tx86W2tTj2m-",
        "colab_type": "code",
        "outputId": "db082ad6-ef0f-48ba-bddc-69d5490ff2ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "apDCM\t DCM_explore1.ipynb  mask\t\t    unet_membrane.hdf5\n",
            "ct_scan  drive\t\t     model-dsbowl2018-1.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ukH1xhWVMfdL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "//start from here\n",
        "import numpy as np \n",
        "import os\n",
        "import skimage.io as io\n",
        "import skimage.transform as trans\n",
        "import numpy as np\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras import backend as keras\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import binary_crossentropy\n",
        "\n",
        "smooth = 1\n",
        "\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n",
        "\n",
        "\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    return  -dice_coef(y_true, y_pred)\n",
        "\n",
        "def unet(pretrained_weights = None,input_size = (512,512,1)):\n",
        "    inputs = Input(input_size)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "    drop4 = Dropout(0.5)(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
        "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
        "    drop5 = Dropout(0.5)(conv5)\n",
        "\n",
        "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
        "    merge6 = concatenate([drop4,up6], axis = 3)\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
        "\n",
        "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
        "    merge7 = concatenate([conv3,up7], axis = 3)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
        "\n",
        "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
        "    merge8 = concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
        "\n",
        "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
        "    merge9 = concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
        "\n",
        "    model = Model(input = inputs, output = conv10)\n",
        "\n",
        "    model.compile(Adam(lr = 1e-4),binary_crossentropy, metrics = ['accuracy',dice_coef])\n",
        "    \n",
        "    #model.summary()\n",
        "\n",
        "    if(pretrained_weights):\n",
        "    \tmodel.load_weights(pretrained_weights)\n",
        "\n",
        "      \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mKWgjgi4rs3V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Scanfolder='./ct_scan/'\n",
        "yfolder='./mask/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3jTqWhpqsUgx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import dicom\n",
        "%matplotlib inline\n",
        "\n",
        "def get_data(Scanfolder, yfolder):\n",
        "    subfolders=os.listdir(Scanfolder)\n",
        "    subfolders.sort()\n",
        "    files=[]\n",
        "    for subfolder in subfolders:\n",
        "        files_names=os.listdir(Scanfolder+subfolder)\n",
        "        files_names.sort()\n",
        "        files_dir=[Scanfolder+subfolder+'/'+names for names in files_names]\n",
        "        files=files+files_dir\n",
        "    X_train=np.array([plt.imread(file) for file in files])\n",
        "    \n",
        "    subfolders=os.listdir(yfolder)\n",
        "    subfolders.sort()\n",
        "    files=[]\n",
        "    for subfolder in subfolders:\n",
        "        files_names=os.listdir(yfolder+subfolder)\n",
        "        files_names.sort()\n",
        "        files_dir=[yfolder+subfolder+'/'+names for names in files_names]\n",
        "        files=files+files_dir\n",
        "\n",
        "    Y_train=np.array([plt.imread(file) for file in files])\n",
        "    return X_train,Y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "720AFbJXsXZc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train,Y_train=get_data(Scanfolder,yfolder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "excaL-Sir6fu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cut_img(X_train,Y_train):\n",
        "    return X_train[:,300:428,200:328],Y_train[:,300:428,200:328] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j__o-DsOsHpi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train,Y_train=cut_img(X_train,Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D-nC-cQNvE_0",
        "colab_type": "code",
        "outputId": "c5477cd2-efb0-4011-e550-00fca5c977cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(X_train[16],cmap=plt.cm.gray)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fdb294c9a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd4AAAHVCAYAAABfWZoAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIYQQYkE08QohhBALoolXCCGEWJD/HwMoOI76OVrTAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ucQm9hG9vI5I",
        "colab_type": "code",
        "outputId": "21a61545-adc1-4406-b35a-9241243d00e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(Y_train[16],cmap=plt.cm.gray)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fdb294a6a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd4AAAHVCAYAAABfWZoAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "tetDKf6Zcjsm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Y_train=Y_train==1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YLt9VFD-co1c",
        "colab_type": "code",
        "outputId": "2cfb457f-18e5-41e6-9145-7d8792ab0ccb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "np.unique(Y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False,  True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "metadata": {
        "id": "obAkxeBSENKx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def resize_data(X_train,Y_train):\n",
        "    X_train=X_train.reshape(X_train.shape[0],X_train.shape[1],X_train.shape[2],1)\n",
        "    Y_train=Y_train.reshape(Y_train.shape[0],Y_train.shape[1],Y_train.shape[2],1)\n",
        "    return X_train,Y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k6wg5aVHEM20",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train,Y_train=resize_data(X_train,Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iQrbGMcbDZd-",
        "colab_type": "code",
        "outputId": "725d422c-b0f4-46d6-f458-6e11d1ef5277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 20488
        }
      },
      "cell_type": "code",
      "source": [
        "model_checkpoint = ModelCheckpoint('unet_membrane.hdf5', monitor='loss',verbose=1, save_best_only=True)\n",
        "model=unet(input_size = (128,128,1))\n",
        "model.fit(X_train,Y_train,validation_split=0.2, batch_size=8,epochs=300,callbacks=[model_checkpoint])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:68: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 100 samples, validate on 26 samples\n",
            "Epoch 1/300\n",
            "100/100 [==============================] - 12s 119ms/step - loss: 0.4149 - acc: 0.9707 - dice_coef: 0.0209 - val_loss: 0.1711 - val_acc: 0.9835 - val_dice_coef: 0.0182\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.41487, saving model to unet_membrane.hdf5\n",
            "Epoch 2/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.1294 - acc: 0.9739 - dice_coef: 0.0319 - val_loss: 0.1688 - val_acc: 0.9835 - val_dice_coef: 0.0299\n",
            "\n",
            "Epoch 00002: loss improved from 0.41487 to 0.12944, saving model to unet_membrane.hdf5\n",
            "Epoch 3/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.1127 - acc: 0.9739 - dice_coef: 0.0536 - val_loss: 0.1041 - val_acc: 0.9835 - val_dice_coef: 0.0226\n",
            "\n",
            "Epoch 00003: loss improved from 0.12944 to 0.11269, saving model to unet_membrane.hdf5\n",
            "Epoch 4/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0971 - acc: 0.9739 - dice_coef: 0.0793 - val_loss: 0.1192 - val_acc: 0.9835 - val_dice_coef: 0.0035\n",
            "\n",
            "Epoch 00004: loss improved from 0.11269 to 0.09706, saving model to unet_membrane.hdf5\n",
            "Epoch 5/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0944 - acc: 0.9739 - dice_coef: 0.0878 - val_loss: 0.0950 - val_acc: 0.9835 - val_dice_coef: 0.0093\n",
            "\n",
            "Epoch 00005: loss improved from 0.09706 to 0.09440, saving model to unet_membrane.hdf5\n",
            "Epoch 6/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0995 - acc: 0.9739 - dice_coef: 0.0814 - val_loss: 0.0870 - val_acc: 0.9835 - val_dice_coef: 0.0288\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.09440\n",
            "Epoch 7/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0879 - acc: 0.9739 - dice_coef: 0.0855 - val_loss: 0.0788 - val_acc: 0.9835 - val_dice_coef: 0.0327\n",
            "\n",
            "Epoch 00007: loss improved from 0.09440 to 0.08788, saving model to unet_membrane.hdf5\n",
            "Epoch 8/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0908 - acc: 0.9739 - dice_coef: 0.1107 - val_loss: 0.0902 - val_acc: 0.9835 - val_dice_coef: 0.0174\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.08788\n",
            "Epoch 9/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0868 - acc: 0.9739 - dice_coef: 0.1156 - val_loss: 0.1126 - val_acc: 0.9835 - val_dice_coef: 0.0068\n",
            "\n",
            "Epoch 00009: loss improved from 0.08788 to 0.08677, saving model to unet_membrane.hdf5\n",
            "Epoch 10/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0932 - acc: 0.9739 - dice_coef: 0.0952 - val_loss: 0.1041 - val_acc: 0.9835 - val_dice_coef: 0.0058\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.08677\n",
            "Epoch 11/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0894 - acc: 0.9739 - dice_coef: 0.0947 - val_loss: 0.0993 - val_acc: 0.9835 - val_dice_coef: 0.0104\n",
            "\n",
            "Epoch 00011: loss did not improve from 0.08677\n",
            "Epoch 12/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0911 - acc: 0.9739 - dice_coef: 0.1034 - val_loss: 0.1185 - val_acc: 0.9835 - val_dice_coef: 0.0053\n",
            "\n",
            "Epoch 00012: loss did not improve from 0.08677\n",
            "Epoch 13/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0867 - acc: 0.9739 - dice_coef: 0.1142 - val_loss: 0.1329 - val_acc: 0.9835 - val_dice_coef: 0.0026\n",
            "\n",
            "Epoch 00013: loss improved from 0.08677 to 0.08671, saving model to unet_membrane.hdf5\n",
            "Epoch 14/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0805 - acc: 0.9739 - dice_coef: 0.1312 - val_loss: 0.1267 - val_acc: 0.9835 - val_dice_coef: 0.0093\n",
            "\n",
            "Epoch 00014: loss improved from 0.08671 to 0.08054, saving model to unet_membrane.hdf5\n",
            "Epoch 15/300\n",
            "100/100 [==============================] - 3s 26ms/step - loss: 0.0737 - acc: 0.9739 - dice_coef: 0.1616 - val_loss: 0.1619 - val_acc: 0.9835 - val_dice_coef: 0.0022\n",
            "\n",
            "Epoch 00015: loss improved from 0.08054 to 0.07373, saving model to unet_membrane.hdf5\n",
            "Epoch 16/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0739 - acc: 0.9739 - dice_coef: 0.1680 - val_loss: 0.2134 - val_acc: 0.9835 - val_dice_coef: 3.6153e-04\n",
            "\n",
            "Epoch 00016: loss did not improve from 0.07373\n",
            "Epoch 17/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0701 - acc: 0.9739 - dice_coef: 0.1667 - val_loss: 0.1423 - val_acc: 0.9835 - val_dice_coef: 0.0026\n",
            "\n",
            "Epoch 00017: loss improved from 0.07373 to 0.07010, saving model to unet_membrane.hdf5\n",
            "Epoch 18/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0826 - acc: 0.9739 - dice_coef: 0.1484 - val_loss: 0.0969 - val_acc: 0.9835 - val_dice_coef: 0.0132\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.07010\n",
            "Epoch 19/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0794 - acc: 0.9739 - dice_coef: 0.1447 - val_loss: 0.1309 - val_acc: 0.9835 - val_dice_coef: 0.0019\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.07010\n",
            "Epoch 20/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0714 - acc: 0.9739 - dice_coef: 0.1857 - val_loss: 0.1596 - val_acc: 0.9835 - val_dice_coef: 3.4312e-04\n",
            "\n",
            "Epoch 00020: loss did not improve from 0.07010\n",
            "Epoch 21/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0798 - acc: 0.9739 - dice_coef: 0.1524 - val_loss: 0.2053 - val_acc: 0.9835 - val_dice_coef: 1.1241e-04\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.07010\n",
            "Epoch 22/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0731 - acc: 0.9739 - dice_coef: 0.1752 - val_loss: 0.2109 - val_acc: 0.9835 - val_dice_coef: 3.1069e-04\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.07010\n",
            "Epoch 23/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0891 - acc: 0.9739 - dice_coef: 0.1442 - val_loss: 0.2473 - val_acc: 0.9835 - val_dice_coef: 2.4059e-06\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.07010\n",
            "Epoch 24/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.1007 - acc: 0.9739 - dice_coef: 0.0842 - val_loss: 0.1306 - val_acc: 0.9835 - val_dice_coef: 0.0082\n",
            "\n",
            "Epoch 00024: loss did not improve from 0.07010\n",
            "Epoch 25/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.1008 - acc: 0.9739 - dice_coef: 0.1177 - val_loss: 0.2266 - val_acc: 0.9835 - val_dice_coef: 3.1086e-05\n",
            "\n",
            "Epoch 00025: loss did not improve from 0.07010\n",
            "Epoch 26/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0818 - acc: 0.9739 - dice_coef: 0.1426 - val_loss: 0.2292 - val_acc: 0.9835 - val_dice_coef: 4.3709e-05\n",
            "\n",
            "Epoch 00026: loss did not improve from 0.07010\n",
            "Epoch 27/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0711 - acc: 0.9739 - dice_coef: 0.1615 - val_loss: 0.2229 - val_acc: 0.9835 - val_dice_coef: 5.3539e-05\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.07010\n",
            "Epoch 28/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0649 - acc: 0.9739 - dice_coef: 0.2015 - val_loss: 0.1960 - val_acc: 0.9835 - val_dice_coef: 8.4852e-04\n",
            "\n",
            "Epoch 00028: loss improved from 0.07010 to 0.06489, saving model to unet_membrane.hdf5\n",
            "Epoch 29/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0583 - acc: 0.9739 - dice_coef: 0.2338 - val_loss: 0.2300 - val_acc: 0.9835 - val_dice_coef: 1.6451e-04\n",
            "\n",
            "Epoch 00029: loss improved from 0.06489 to 0.05829, saving model to unet_membrane.hdf5\n",
            "Epoch 30/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0716 - acc: 0.9739 - dice_coef: 0.1909 - val_loss: 0.2609 - val_acc: 0.9835 - val_dice_coef: 2.3529e-05\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.05829\n",
            "Epoch 31/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0688 - acc: 0.9739 - dice_coef: 0.2316 - val_loss: 0.2785 - val_acc: 0.9835 - val_dice_coef: 4.6079e-07\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.05829\n",
            "Epoch 32/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0569 - acc: 0.9739 - dice_coef: 0.2360 - val_loss: 0.2583 - val_acc: 0.9835 - val_dice_coef: 6.2177e-05\n",
            "\n",
            "Epoch 00032: loss improved from 0.05829 to 0.05685, saving model to unet_membrane.hdf5\n",
            "Epoch 33/300\n",
            "100/100 [==============================] - 3s 26ms/step - loss: 0.0625 - acc: 0.9739 - dice_coef: 0.2357 - val_loss: 0.2428 - val_acc: 0.9835 - val_dice_coef: 0.0013\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.05685\n",
            "Epoch 34/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0528 - acc: 0.9739 - dice_coef: 0.3143 - val_loss: 0.2686 - val_acc: 0.9835 - val_dice_coef: 2.7604e-06\n",
            "\n",
            "Epoch 00034: loss improved from 0.05685 to 0.05277, saving model to unet_membrane.hdf5\n",
            "Epoch 35/300\n",
            "100/100 [==============================] - 3s 26ms/step - loss: 0.0522 - acc: 0.9739 - dice_coef: 0.2920 - val_loss: 0.2653 - val_acc: 0.9835 - val_dice_coef: 1.6352e-05\n",
            "\n",
            "Epoch 00035: loss improved from 0.05277 to 0.05215, saving model to unet_membrane.hdf5\n",
            "Epoch 36/300\n",
            "100/100 [==============================] - 3s 26ms/step - loss: 0.0444 - acc: 0.9739 - dice_coef: 0.3566 - val_loss: 0.2695 - val_acc: 0.9835 - val_dice_coef: 3.3249e-07\n",
            "\n",
            "Epoch 00036: loss improved from 0.05215 to 0.04435, saving model to unet_membrane.hdf5\n",
            "Epoch 37/300\n",
            "100/100 [==============================] - 3s 26ms/step - loss: 0.0416 - acc: 0.9739 - dice_coef: 0.4178 - val_loss: 0.2440 - val_acc: 0.9835 - val_dice_coef: 4.0583e-04\n",
            "\n",
            "Epoch 00037: loss improved from 0.04435 to 0.04158, saving model to unet_membrane.hdf5\n",
            "Epoch 38/300\n",
            "100/100 [==============================] - 3s 26ms/step - loss: 0.0420 - acc: 0.9739 - dice_coef: 0.3682 - val_loss: 0.2603 - val_acc: 0.9835 - val_dice_coef: 2.9831e-05\n",
            "\n",
            "Epoch 00038: loss did not improve from 0.04158\n",
            "Epoch 39/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0514 - acc: 0.9739 - dice_coef: 0.2844 - val_loss: 0.2569 - val_acc: 0.9835 - val_dice_coef: 5.8158e-05\n",
            "\n",
            "Epoch 00039: loss did not improve from 0.04158\n",
            "Epoch 40/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0439 - acc: 0.9739 - dice_coef: 0.3789 - val_loss: 0.2762 - val_acc: 0.9835 - val_dice_coef: 2.0764e-09\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.04158\n",
            "Epoch 41/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0397 - acc: 0.9739 - dice_coef: 0.3801 - val_loss: 0.2627 - val_acc: 0.9835 - val_dice_coef: 4.2439e-06\n",
            "\n",
            "Epoch 00041: loss improved from 0.04158 to 0.03966, saving model to unet_membrane.hdf5\n",
            "Epoch 42/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0389 - acc: 0.9739 - dice_coef: 0.4228 - val_loss: 0.2562 - val_acc: 0.9835 - val_dice_coef: 1.2702e-04\n",
            "\n",
            "Epoch 00042: loss improved from 0.03966 to 0.03893, saving model to unet_membrane.hdf5\n",
            "Epoch 43/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0382 - acc: 0.9739 - dice_coef: 0.4217 - val_loss: 0.2724 - val_acc: 0.9835 - val_dice_coef: 8.3574e-08\n",
            "\n",
            "Epoch 00043: loss improved from 0.03893 to 0.03817, saving model to unet_membrane.hdf5\n",
            "Epoch 44/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0342 - acc: 0.9739 - dice_coef: 0.4535 - val_loss: 0.2650 - val_acc: 0.9835 - val_dice_coef: 2.2204e-06\n",
            "\n",
            "Epoch 00044: loss improved from 0.03817 to 0.03423, saving model to unet_membrane.hdf5\n",
            "Epoch 45/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0324 - acc: 0.9739 - dice_coef: 0.4989 - val_loss: 0.2658 - val_acc: 0.9835 - val_dice_coef: 8.8928e-07\n",
            "\n",
            "Epoch 00045: loss improved from 0.03423 to 0.03245, saving model to unet_membrane.hdf5\n",
            "Epoch 46/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0323 - acc: 0.9739 - dice_coef: 0.4304 - val_loss: 0.2690 - val_acc: 0.9835 - val_dice_coef: 2.1521e-06\n",
            "\n",
            "Epoch 00046: loss improved from 0.03245 to 0.03226, saving model to unet_membrane.hdf5\n",
            "Epoch 47/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0373 - acc: 0.9739 - dice_coef: 0.4451 - val_loss: 0.2598 - val_acc: 0.9835 - val_dice_coef: 0.0015\n",
            "\n",
            "Epoch 00047: loss did not improve from 0.03226\n",
            "Epoch 48/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0416 - acc: 0.9739 - dice_coef: 0.4062 - val_loss: 0.2204 - val_acc: 0.9835 - val_dice_coef: 0.0023\n",
            "\n",
            "Epoch 00048: loss did not improve from 0.03226\n",
            "Epoch 49/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0392 - acc: 0.9860 - dice_coef: 0.4001 - val_loss: 0.2440 - val_acc: 0.9814 - val_dice_coef: 0.0078\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.03226\n",
            "Epoch 50/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0473 - acc: 0.9820 - dice_coef: 0.3587 - val_loss: 0.2505 - val_acc: 0.9692 - val_dice_coef: 0.0019\n",
            "\n",
            "Epoch 00050: loss did not improve from 0.03226\n",
            "Epoch 51/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0463 - acc: 0.9809 - dice_coef: 0.4068 - val_loss: 0.2588 - val_acc: 0.9835 - val_dice_coef: 7.1697e-06\n",
            "\n",
            "Epoch 00051: loss did not improve from 0.03226\n",
            "Epoch 52/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0406 - acc: 0.9854 - dice_coef: 0.4002 - val_loss: 0.2416 - val_acc: 0.9822 - val_dice_coef: 7.9394e-04\n",
            "\n",
            "Epoch 00052: loss did not improve from 0.03226\n",
            "Epoch 53/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0321 - acc: 0.9888 - dice_coef: 0.4787 - val_loss: 0.2563 - val_acc: 0.9835 - val_dice_coef: 1.4984e-04\n",
            "\n",
            "Epoch 00053: loss improved from 0.03226 to 0.03205, saving model to unet_membrane.hdf5\n",
            "Epoch 54/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0284 - acc: 0.9916 - dice_coef: 0.4959 - val_loss: 0.2579 - val_acc: 0.9809 - val_dice_coef: 3.9113e-05\n",
            "\n",
            "Epoch 00054: loss improved from 0.03205 to 0.02836, saving model to unet_membrane.hdf5\n",
            "Epoch 55/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0331 - acc: 0.9892 - dice_coef: 0.4926 - val_loss: 0.2690 - val_acc: 0.9829 - val_dice_coef: 2.6077e-08\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.02836\n",
            "Epoch 56/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0320 - acc: 0.9902 - dice_coef: 0.4313 - val_loss: 0.2639 - val_acc: 0.9835 - val_dice_coef: 1.1548e-05\n",
            "\n",
            "Epoch 00056: loss did not improve from 0.02836\n",
            "Epoch 57/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0399 - acc: 0.9868 - dice_coef: 0.4112 - val_loss: 0.2631 - val_acc: 0.9835 - val_dice_coef: 1.6397e-06\n",
            "\n",
            "Epoch 00057: loss did not improve from 0.02836\n",
            "Epoch 58/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0349 - acc: 0.9877 - dice_coef: 0.4410 - val_loss: 0.2667 - val_acc: 0.9831 - val_dice_coef: 3.3691e-06\n",
            "\n",
            "Epoch 00058: loss did not improve from 0.02836\n",
            "Epoch 59/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0301 - acc: 0.9907 - dice_coef: 0.4522 - val_loss: 0.2681 - val_acc: 0.9826 - val_dice_coef: 2.5739e-07\n",
            "\n",
            "Epoch 00059: loss did not improve from 0.02836\n",
            "Epoch 60/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0295 - acc: 0.9917 - dice_coef: 0.4598 - val_loss: 0.2674 - val_acc: 0.9831 - val_dice_coef: 1.0481e-07\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.02836\n",
            "Epoch 61/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0335 - acc: 0.9889 - dice_coef: 0.4778 - val_loss: 0.2722 - val_acc: 0.9745 - val_dice_coef: 5.4491e-06\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.02836\n",
            "Epoch 62/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0375 - acc: 0.9890 - dice_coef: 0.4548 - val_loss: 0.2657 - val_acc: 0.9835 - val_dice_coef: 2.7603e-07\n",
            "\n",
            "Epoch 00062: loss did not improve from 0.02836\n",
            "Epoch 63/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0312 - acc: 0.9896 - dice_coef: 0.4122 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 7.7712e-08\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.02836\n",
            "Epoch 64/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0319 - acc: 0.9895 - dice_coef: 0.4622 - val_loss: 0.2672 - val_acc: 0.9835 - val_dice_coef: 1.6167e-08\n",
            "\n",
            "Epoch 00064: loss did not improve from 0.02836\n",
            "Epoch 65/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0272 - acc: 0.9922 - dice_coef: 0.5565 - val_loss: 0.2673 - val_acc: 0.9835 - val_dice_coef: 1.0608e-09\n",
            "\n",
            "Epoch 00065: loss improved from 0.02836 to 0.02718, saving model to unet_membrane.hdf5\n",
            "Epoch 66/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0248 - acc: 0.9939 - dice_coef: 0.5797 - val_loss: 0.2662 - val_acc: 0.9835 - val_dice_coef: 8.3462e-09\n",
            "\n",
            "Epoch 00066: loss improved from 0.02718 to 0.02482, saving model to unet_membrane.hdf5\n",
            "Epoch 67/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0259 - acc: 0.9933 - dice_coef: 0.4753 - val_loss: 0.2673 - val_acc: 0.9831 - val_dice_coef: 8.4394e-08\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.02482\n",
            "Epoch 68/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0257 - acc: 0.9933 - dice_coef: 0.5836 - val_loss: 0.2683 - val_acc: 0.9826 - val_dice_coef: 1.4042e-10\n",
            "\n",
            "Epoch 00068: loss did not improve from 0.02482\n",
            "Epoch 69/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0245 - acc: 0.9944 - dice_coef: 0.4988 - val_loss: 0.2663 - val_acc: 0.9835 - val_dice_coef: 3.3000e-08\n",
            "\n",
            "Epoch 00069: loss improved from 0.02482 to 0.02453, saving model to unet_membrane.hdf5\n",
            "Epoch 70/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0249 - acc: 0.9941 - dice_coef: 0.5382 - val_loss: 0.2661 - val_acc: 0.9835 - val_dice_coef: 2.7195e-08\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.02453\n",
            "Epoch 71/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0269 - acc: 0.9926 - dice_coef: 0.5285 - val_loss: 0.2662 - val_acc: 0.9835 - val_dice_coef: 1.8056e-10\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.02453\n",
            "Epoch 72/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0268 - acc: 0.9926 - dice_coef: 0.5229 - val_loss: 0.2669 - val_acc: 0.9835 - val_dice_coef: 2.6632e-10\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.02453\n",
            "Epoch 73/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0245 - acc: 0.9943 - dice_coef: 0.5330 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 3.0704e-06\n",
            "\n",
            "Epoch 00073: loss improved from 0.02453 to 0.02451, saving model to unet_membrane.hdf5\n",
            "Epoch 74/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0242 - acc: 0.9949 - dice_coef: 0.5444 - val_loss: 0.2661 - val_acc: 0.9835 - val_dice_coef: 1.1420e-09\n",
            "\n",
            "Epoch 00074: loss improved from 0.02451 to 0.02421, saving model to unet_membrane.hdf5\n",
            "Epoch 75/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0233 - acc: 0.9954 - dice_coef: 0.4875 - val_loss: 0.2667 - val_acc: 0.9835 - val_dice_coef: 2.0132e-09\n",
            "\n",
            "Epoch 00075: loss improved from 0.02421 to 0.02329, saving model to unet_membrane.hdf5\n",
            "Epoch 76/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0241 - acc: 0.9946 - dice_coef: 0.4739 - val_loss: 0.2663 - val_acc: 0.9835 - val_dice_coef: 4.0174e-10\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.02329\n",
            "Epoch 77/300\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 0.0231 - acc: 0.9959 - dice_coef: 0.6053 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 1.2594e-09\n",
            "\n",
            "Epoch 00077: loss improved from 0.02329 to 0.02308, saving model to unet_membrane.hdf5\n",
            "Epoch 78/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0238 - acc: 0.9948 - dice_coef: 0.5570 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 2.6040e-08\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.02308\n",
            "Epoch 79/300\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 0.0232 - acc: 0.9954 - dice_coef: 0.5577 - val_loss: 0.2654 - val_acc: 0.9835 - val_dice_coef: 8.8665e-07\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.02308\n",
            "Epoch 80/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0235 - acc: 0.9951 - dice_coef: 0.5588 - val_loss: 0.2654 - val_acc: 0.9835 - val_dice_coef: 7.3348e-07\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.02308\n",
            "Epoch 81/300\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 0.0237 - acc: 0.9950 - dice_coef: 0.5492 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 3.6573e-08\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.02308\n",
            "Epoch 82/300\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 0.0228 - acc: 0.9957 - dice_coef: 0.5060 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 5.7883e-09\n",
            "\n",
            "Epoch 00082: loss improved from 0.02308 to 0.02276, saving model to unet_membrane.hdf5\n",
            "Epoch 83/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0219 - acc: 0.9963 - dice_coef: 0.6176 - val_loss: 0.2657 - val_acc: 0.9835 - val_dice_coef: 9.1704e-08\n",
            "\n",
            "Epoch 00083: loss improved from 0.02276 to 0.02194, saving model to unet_membrane.hdf5\n",
            "Epoch 84/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0217 - acc: 0.9964 - dice_coef: 0.5690 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 3.8682e-09\n",
            "\n",
            "Epoch 00084: loss improved from 0.02194 to 0.02175, saving model to unet_membrane.hdf5\n",
            "Epoch 85/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0217 - acc: 0.9965 - dice_coef: 0.6001 - val_loss: 0.2658 - val_acc: 0.9835 - val_dice_coef: 1.6866e-08\n",
            "\n",
            "Epoch 00085: loss improved from 0.02175 to 0.02168, saving model to unet_membrane.hdf5\n",
            "Epoch 86/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0214 - acc: 0.9967 - dice_coef: 0.6227 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 9.7112e-09\n",
            "\n",
            "Epoch 00086: loss improved from 0.02168 to 0.02139, saving model to unet_membrane.hdf5\n",
            "Epoch 87/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0218 - acc: 0.9964 - dice_coef: 0.5948 - val_loss: 0.2661 - val_acc: 0.9835 - val_dice_coef: 2.7083e-10\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.02139\n",
            "Epoch 88/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0212 - acc: 0.9967 - dice_coef: 0.5538 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 2.7986e-10\n",
            "\n",
            "Epoch 00088: loss improved from 0.02139 to 0.02121, saving model to unet_membrane.hdf5\n",
            "Epoch 89/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0212 - acc: 0.9969 - dice_coef: 0.5771 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 3.0694e-10\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.02121\n",
            "Epoch 90/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0214 - acc: 0.9968 - dice_coef: 0.5777 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 2.9792e-10\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.02121\n",
            "Epoch 91/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0215 - acc: 0.9966 - dice_coef: 0.6236 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 3.5658e-09\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.02121\n",
            "Epoch 92/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0216 - acc: 0.9968 - dice_coef: 0.4748 - val_loss: 0.2661 - val_acc: 0.9835 - val_dice_coef: 9.6646e-10\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.02121\n",
            "Epoch 93/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0215 - acc: 0.9967 - dice_coef: 0.5250 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 7.8708e-10\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.02121\n",
            "Epoch 94/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0211 - acc: 0.9969 - dice_coef: 0.5541 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 5.4618e-10\n",
            "\n",
            "Epoch 00094: loss improved from 0.02121 to 0.02108, saving model to unet_membrane.hdf5\n",
            "Epoch 95/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0208 - acc: 0.9973 - dice_coef: 0.5317 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00095: loss improved from 0.02108 to 0.02083, saving model to unet_membrane.hdf5\n",
            "Epoch 96/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0216 - acc: 0.9969 - dice_coef: 0.5570 - val_loss: 0.2661 - val_acc: 0.9835 - val_dice_coef: 9.9305e-11\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.02083\n",
            "Epoch 97/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0229 - acc: 0.9954 - dice_coef: 0.5861 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 1.5644e-09\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.02083\n",
            "Epoch 98/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0218 - acc: 0.9965 - dice_coef: 0.5236 - val_loss: 0.2661 - val_acc: 0.9835 - val_dice_coef: 1.7604e-10\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.02083\n",
            "Epoch 99/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0220 - acc: 0.9961 - dice_coef: 0.6163 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 3.6111e-11\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.02083\n",
            "Epoch 100/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0211 - acc: 0.9970 - dice_coef: 0.4550 - val_loss: 0.2657 - val_acc: 0.9835 - val_dice_coef: 3.9386e-08\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.02083\n",
            "Epoch 101/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0216 - acc: 0.9965 - dice_coef: 0.4908 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 8.2144e-09\n",
            "\n",
            "Epoch 00101: loss did not improve from 0.02083\n",
            "Epoch 102/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0209 - acc: 0.9972 - dice_coef: 0.5585 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 1.6628e-09\n",
            "\n",
            "Epoch 00102: loss did not improve from 0.02083\n",
            "Epoch 103/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0207 - acc: 0.9974 - dice_coef: 0.5611 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 2.0120e-09\n",
            "\n",
            "Epoch 00103: loss improved from 0.02083 to 0.02072, saving model to unet_membrane.hdf5\n",
            "Epoch 104/300\n",
            "100/100 [==============================] - 3s 26ms/step - loss: 0.0210 - acc: 0.9971 - dice_coef: 0.5064 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 5.2361e-10\n",
            "\n",
            "Epoch 00104: loss did not improve from 0.02072\n",
            "Epoch 105/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0211 - acc: 0.9970 - dice_coef: 0.5782 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 8.1250e-11\n",
            "\n",
            "Epoch 00105: loss did not improve from 0.02072\n",
            "Epoch 106/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0208 - acc: 0.9972 - dice_coef: 0.5548 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 2.2118e-10\n",
            "\n",
            "Epoch 00106: loss did not improve from 0.02072\n",
            "Epoch 107/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0206 - acc: 0.9974 - dice_coef: 0.5859 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 6.7708e-11\n",
            "\n",
            "Epoch 00107: loss improved from 0.02072 to 0.02061, saving model to unet_membrane.hdf5\n",
            "Epoch 108/300\n",
            "100/100 [==============================] - 3s 26ms/step - loss: 0.0208 - acc: 0.9973 - dice_coef: 0.5828 - val_loss: 0.2661 - val_acc: 0.9835 - val_dice_coef: 3.9271e-10\n",
            "\n",
            "Epoch 00108: loss did not improve from 0.02061\n",
            "Epoch 109/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0211 - acc: 0.9970 - dice_coef: 0.5516 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: nan\n",
            "\n",
            "Epoch 00109: loss did not improve from 0.02061\n",
            "Epoch 110/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0212 - acc: 0.9968 - dice_coef: 0.5830 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: nan\n",
            "\n",
            "Epoch 00110: loss did not improve from 0.02061\n",
            "Epoch 111/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0212 - acc: 0.9967 - dice_coef: 0.5769 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 1.8056e-11\n",
            "\n",
            "Epoch 00111: loss did not improve from 0.02061\n",
            "Epoch 112/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0206 - acc: 0.9973 - dice_coef: 0.5572 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00112: loss improved from 0.02061 to 0.02061, saving model to unet_membrane.hdf5\n",
            "Epoch 113/300\n",
            "100/100 [==============================] - 3s 26ms/step - loss: 0.0206 - acc: 0.9973 - dice_coef: 0.5624 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: nan\n",
            "\n",
            "Epoch 00113: loss did not improve from 0.02061\n",
            "Epoch 114/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0217 - acc: 0.9966 - dice_coef: 0.6234 - val_loss: 0.2663 - val_acc: 0.9835 - val_dice_coef: nan\n",
            "\n",
            "Epoch 00114: loss did not improve from 0.02061\n",
            "Epoch 115/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0213 - acc: 0.9967 - dice_coef: 0.5817 - val_loss: 0.2661 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00115: loss did not improve from 0.02061\n",
            "Epoch 116/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0208 - acc: 0.9971 - dice_coef: 0.5830 - val_loss: 0.2661 - val_acc: 0.9835 - val_dice_coef: nan\n",
            "\n",
            "Epoch 00116: loss did not improve from 0.02061\n",
            "Epoch 117/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0206 - acc: 0.9973 - dice_coef: 0.5118 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00117: loss improved from 0.02061 to 0.02057, saving model to unet_membrane.hdf5\n",
            "Epoch 118/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0204 - acc: 0.9974 - dice_coef: 0.5897 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: nan\n",
            "\n",
            "Epoch 00118: loss improved from 0.02057 to 0.02040, saving model to unet_membrane.hdf5\n",
            "Epoch 119/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0202 - acc: 0.9977 - dice_coef: 0.6418 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 2.7083e-11\n",
            "\n",
            "Epoch 00119: loss improved from 0.02040 to 0.02024, saving model to unet_membrane.hdf5\n",
            "Epoch 120/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0201 - acc: 0.9977 - dice_coef: 0.5914 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 4.7140e-10\n",
            "\n",
            "Epoch 00120: loss improved from 0.02024 to 0.02014, saving model to unet_membrane.hdf5\n",
            "Epoch 121/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0199 - acc: 0.9979 - dice_coef: 0.5945 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 5.5834e-11\n",
            "\n",
            "Epoch 00121: loss improved from 0.02014 to 0.01993, saving model to unet_membrane.hdf5\n",
            "Epoch 122/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0200 - acc: 0.9978 - dice_coef: 0.6182 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 1.2403e-10\n",
            "\n",
            "Epoch 00122: loss did not improve from 0.01993\n",
            "Epoch 123/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0205 - acc: 0.9975 - dice_coef: 0.5883 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00123: loss did not improve from 0.01993\n",
            "Epoch 124/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0209 - acc: 0.9972 - dice_coef: 0.6086 - val_loss: 0.2653 - val_acc: 0.9835 - val_dice_coef: 5.4973e-08\n",
            "\n",
            "Epoch 00124: loss did not improve from 0.01993\n",
            "Epoch 125/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0208 - acc: 0.9972 - dice_coef: 0.5594 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 4.5139e-11\n",
            "\n",
            "Epoch 00125: loss did not improve from 0.01993\n",
            "Epoch 126/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0211 - acc: 0.9970 - dice_coef: 0.5848 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00126: loss did not improve from 0.01993\n",
            "Epoch 127/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0206 - acc: 0.9973 - dice_coef: 0.6144 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: nan\n",
            "\n",
            "Epoch 00127: loss did not improve from 0.01993\n",
            "Epoch 128/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0215 - acc: 0.9965 - dice_coef: 0.5777 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 1.7958e-09\n",
            "\n",
            "Epoch 00128: loss did not improve from 0.01993\n",
            "Epoch 129/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0206 - acc: 0.9971 - dice_coef: 0.6397 - val_loss: 0.2665 - val_acc: 0.9835 - val_dice_coef: 2.7960e-09\n",
            "\n",
            "Epoch 00129: loss did not improve from 0.01993\n",
            "Epoch 130/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0205 - acc: 0.9975 - dice_coef: 0.5122 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00130: loss did not improve from 0.01993\n",
            "Epoch 131/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0203 - acc: 0.9976 - dice_coef: 0.5369 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 4.9653e-11\n",
            "\n",
            "Epoch 00131: loss did not improve from 0.01993\n",
            "Epoch 132/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0203 - acc: 0.9975 - dice_coef: 0.5909 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 2.0656e-09\n",
            "\n",
            "Epoch 00132: loss did not improve from 0.01993\n",
            "Epoch 133/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0199 - acc: 0.9979 - dice_coef: 0.4644 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00133: loss improved from 0.01993 to 0.01989, saving model to unet_membrane.hdf5\n",
            "Epoch 134/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0197 - acc: 0.9981 - dice_coef: 0.5969 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00134: loss improved from 0.01989 to 0.01972, saving model to unet_membrane.hdf5\n",
            "Epoch 135/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0197 - acc: 0.9980 - dice_coef: 0.5441 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: nan\n",
            "\n",
            "Epoch 00135: loss improved from 0.01972 to 0.01970, saving model to unet_membrane.hdf5\n",
            "Epoch 136/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0196 - acc: 0.9981 - dice_coef: 0.5986 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00136: loss improved from 0.01970 to 0.01956, saving model to unet_membrane.hdf5\n",
            "Epoch 137/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0196 - acc: 0.9981 - dice_coef: 0.5469 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: nan\n",
            "\n",
            "Epoch 00137: loss did not improve from 0.01956\n",
            "Epoch 138/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0195 - acc: 0.9982 - dice_coef: 0.6239 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: nan\n",
            "\n",
            "Epoch 00138: loss improved from 0.01956 to 0.01947, saving model to unet_membrane.hdf5\n",
            "Epoch 139/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0195 - acc: 0.9981 - dice_coef: 0.5473 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: nan\n",
            "\n",
            "Epoch 00139: loss did not improve from 0.01947\n",
            "Epoch 140/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0198 - acc: 0.9979 - dice_coef: 0.5965 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: nan\n",
            "\n",
            "Epoch 00140: loss did not improve from 0.01947\n",
            "Epoch 141/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0197 - acc: 0.9978 - dice_coef: 0.5983 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: nan\n",
            "\n",
            "Epoch 00141: loss did not improve from 0.01947\n",
            "Epoch 142/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0199 - acc: 0.9978 - dice_coef: 0.5698 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 9.0278e-12\n",
            "\n",
            "Epoch 00142: loss did not improve from 0.01947\n",
            "Epoch 143/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0204 - acc: 0.9975 - dice_coef: 0.4895 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 1.0382e-10\n",
            "\n",
            "Epoch 00143: loss did not improve from 0.01947\n",
            "Epoch 144/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0201 - acc: 0.9976 - dice_coef: 0.5413 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: nan\n",
            "\n",
            "Epoch 00144: loss did not improve from 0.01947\n",
            "Epoch 145/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0197 - acc: 0.9980 - dice_coef: 0.5949 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00145: loss did not improve from 0.01947\n",
            "Epoch 146/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0197 - acc: 0.9979 - dice_coef: 0.5470 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00146: loss did not improve from 0.01947\n",
            "Epoch 147/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0198 - acc: 0.9979 - dice_coef: 0.5980 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00147: loss did not improve from 0.01947\n",
            "Epoch 148/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0197 - acc: 0.9978 - dice_coef: 0.5976 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 2.0312e-10\n",
            "\n",
            "Epoch 00148: loss did not improve from 0.01947\n",
            "Epoch 149/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0204 - acc: 0.9974 - dice_coef: 0.5902 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 1.8055e-11\n",
            "\n",
            "Epoch 00149: loss did not improve from 0.01947\n",
            "Epoch 150/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0198 - acc: 0.9977 - dice_coef: 0.6463 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00150: loss did not improve from 0.01947\n",
            "Epoch 151/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0194 - acc: 0.9981 - dice_coef: 0.6533 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00151: loss improved from 0.01947 to 0.01942, saving model to unet_membrane.hdf5\n",
            "Epoch 152/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0193 - acc: 0.9981 - dice_coef: 0.6548 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00152: loss improved from 0.01942 to 0.01932, saving model to unet_membrane.hdf5\n",
            "Epoch 153/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0193 - acc: 0.9982 - dice_coef: 0.6555 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00153: loss improved from 0.01932 to 0.01930, saving model to unet_membrane.hdf5\n",
            "Epoch 154/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0194 - acc: 0.9982 - dice_coef: 0.5998 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00154: loss did not improve from 0.01930\n",
            "Epoch 155/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0192 - acc: 0.9982 - dice_coef: 0.5508 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00155: loss improved from 0.01930 to 0.01915, saving model to unet_membrane.hdf5\n",
            "Epoch 156/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0191 - acc: 0.9983 - dice_coef: 0.6303 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00156: loss improved from 0.01915 to 0.01910, saving model to unet_membrane.hdf5\n",
            "Epoch 157/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0192 - acc: 0.9983 - dice_coef: 0.6566 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00157: loss did not improve from 0.01910\n",
            "Epoch 158/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0191 - acc: 0.9984 - dice_coef: 0.6309 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00158: loss improved from 0.01910 to 0.01905, saving model to unet_membrane.hdf5\n",
            "Epoch 159/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0190 - acc: 0.9984 - dice_coef: 0.5536 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: nan\n",
            "\n",
            "Epoch 00159: loss improved from 0.01905 to 0.01904, saving model to unet_membrane.hdf5\n",
            "Epoch 160/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0192 - acc: 0.9982 - dice_coef: 0.6291 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: nan\n",
            "\n",
            "Epoch 00160: loss did not improve from 0.01904\n",
            "Epoch 161/300\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 0.0192 - acc: 0.9983 - dice_coef: 0.6570 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 8.5764e-11\n",
            "\n",
            "Epoch 00161: loss did not improve from 0.01904\n",
            "Epoch 162/300\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 0.0192 - acc: 0.9983 - dice_coef: 0.6048 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: nan\n",
            "\n",
            "Epoch 00162: loss did not improve from 0.01904\n",
            "Epoch 163/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0191 - acc: 0.9982 - dice_coef: 0.5535 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00163: loss did not improve from 0.01904\n",
            "Epoch 164/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0192 - acc: 0.9983 - dice_coef: 0.5524 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: nan\n",
            "\n",
            "Epoch 00164: loss did not improve from 0.01904\n",
            "Epoch 165/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0192 - acc: 0.9983 - dice_coef: 0.5792 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00165: loss did not improve from 0.01904\n",
            "Epoch 166/300\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 0.0194 - acc: 0.9982 - dice_coef: 0.5516 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00166: loss did not improve from 0.01904\n",
            "Epoch 167/300\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 0.0194 - acc: 0.9980 - dice_coef: 0.6292 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00167: loss did not improve from 0.01904\n",
            "Epoch 168/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0193 - acc: 0.9981 - dice_coef: 0.6574 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00168: loss did not improve from 0.01904\n",
            "Epoch 169/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0191 - acc: 0.9982 - dice_coef: 0.6570 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00169: loss did not improve from 0.01904\n",
            "Epoch 170/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0197 - acc: 0.9979 - dice_coef: 0.6534 - val_loss: 0.2661 - val_acc: 0.9835 - val_dice_coef: 1.3339e-08\n",
            "\n",
            "Epoch 00170: loss did not improve from 0.01904\n",
            "Epoch 171/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0199 - acc: 0.9976 - dice_coef: 0.5999 - val_loss: 0.2658 - val_acc: 0.9835 - val_dice_coef: 2.3163e-08\n",
            "\n",
            "Epoch 00171: loss did not improve from 0.01904\n",
            "Epoch 172/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0197 - acc: 0.9978 - dice_coef: 0.5473 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00172: loss did not improve from 0.01904\n",
            "Epoch 173/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0196 - acc: 0.9979 - dice_coef: 0.5998 - val_loss: 0.2661 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00173: loss did not improve from 0.01904\n",
            "Epoch 174/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0197 - acc: 0.9979 - dice_coef: 0.6541 - val_loss: 0.2665 - val_acc: 0.9834 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00174: loss did not improve from 0.01904\n",
            "Epoch 175/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0246 - acc: 0.9935 - dice_coef: 0.5077 - val_loss: 0.2671 - val_acc: 0.9829 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00175: loss did not improve from 0.01904\n",
            "Epoch 176/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0259 - acc: 0.9923 - dice_coef: 0.4931 - val_loss: 0.2785 - val_acc: 0.9717 - val_dice_coef: 6.3224e-07\n",
            "\n",
            "Epoch 00176: loss did not improve from 0.01904\n",
            "Epoch 177/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0277 - acc: 0.9927 - dice_coef: 0.5438 - val_loss: 0.2547 - val_acc: 0.9835 - val_dice_coef: 2.5058e-04\n",
            "\n",
            "Epoch 00177: loss did not improve from 0.01904\n",
            "Epoch 178/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0356 - acc: 0.9873 - dice_coef: 0.4844 - val_loss: 0.2547 - val_acc: 0.9835 - val_dice_coef: 0.0020\n",
            "\n",
            "Epoch 00178: loss did not improve from 0.01904\n",
            "Epoch 179/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0489 - acc: 0.9821 - dice_coef: 0.4143 - val_loss: 0.2384 - val_acc: 0.9835 - val_dice_coef: 1.0953e-04\n",
            "\n",
            "Epoch 00179: loss did not improve from 0.01904\n",
            "Epoch 180/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0427 - acc: 0.9867 - dice_coef: 0.4300 - val_loss: 0.2342 - val_acc: 0.9835 - val_dice_coef: 0.0076\n",
            "\n",
            "Epoch 00180: loss did not improve from 0.01904\n",
            "Epoch 181/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0401 - acc: 0.9841 - dice_coef: 0.4757 - val_loss: 0.2162 - val_acc: 0.9835 - val_dice_coef: 0.0078\n",
            "\n",
            "Epoch 00181: loss did not improve from 0.01904\n",
            "Epoch 182/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0321 - acc: 0.9902 - dice_coef: 0.4942 - val_loss: 0.2550 - val_acc: 0.9835 - val_dice_coef: 0.0013\n",
            "\n",
            "Epoch 00182: loss did not improve from 0.01904\n",
            "Epoch 183/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0383 - acc: 0.9870 - dice_coef: 0.4900 - val_loss: 0.2538 - val_acc: 0.9835 - val_dice_coef: 1.3042e-05\n",
            "\n",
            "Epoch 00183: loss did not improve from 0.01904\n",
            "Epoch 184/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0360 - acc: 0.9856 - dice_coef: 0.4682 - val_loss: 0.2577 - val_acc: 0.9835 - val_dice_coef: 3.5495e-04\n",
            "\n",
            "Epoch 00184: loss did not improve from 0.01904\n",
            "Epoch 185/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0392 - acc: 0.9866 - dice_coef: 0.4068 - val_loss: 0.2226 - val_acc: 0.9830 - val_dice_coef: 0.0311\n",
            "\n",
            "Epoch 00185: loss did not improve from 0.01904\n",
            "Epoch 186/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0337 - acc: 0.9873 - dice_coef: 0.5068 - val_loss: 0.2279 - val_acc: 0.9835 - val_dice_coef: 0.0092\n",
            "\n",
            "Epoch 00186: loss did not improve from 0.01904\n",
            "Epoch 187/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0281 - acc: 0.9913 - dice_coef: 0.5199 - val_loss: 0.2563 - val_acc: 0.9835 - val_dice_coef: 5.7986e-04\n",
            "\n",
            "Epoch 00187: loss did not improve from 0.01904\n",
            "Epoch 188/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0284 - acc: 0.9910 - dice_coef: 0.5185 - val_loss: 0.2395 - val_acc: 0.9835 - val_dice_coef: 0.0031\n",
            "\n",
            "Epoch 00188: loss did not improve from 0.01904\n",
            "Epoch 189/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0264 - acc: 0.9933 - dice_coef: 0.4914 - val_loss: 0.2496 - val_acc: 0.9835 - val_dice_coef: 8.7562e-05\n",
            "\n",
            "Epoch 00189: loss did not improve from 0.01904\n",
            "Epoch 190/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0233 - acc: 0.9950 - dice_coef: 0.5608 - val_loss: 0.2604 - val_acc: 0.9835 - val_dice_coef: 1.3896e-05\n",
            "\n",
            "Epoch 00190: loss did not improve from 0.01904\n",
            "Epoch 191/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0220 - acc: 0.9959 - dice_coef: 0.6233 - val_loss: 0.2599 - val_acc: 0.9835 - val_dice_coef: 1.5596e-05\n",
            "\n",
            "Epoch 00191: loss did not improve from 0.01904\n",
            "Epoch 192/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0218 - acc: 0.9962 - dice_coef: 0.4309 - val_loss: 0.2626 - val_acc: 0.9835 - val_dice_coef: 2.1008e-06\n",
            "\n",
            "Epoch 00192: loss did not improve from 0.01904\n",
            "Epoch 193/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0263 - acc: 0.9918 - dice_coef: 0.4903 - val_loss: 0.2622 - val_acc: 0.9835 - val_dice_coef: 2.8935e-06\n",
            "\n",
            "Epoch 00193: loss did not improve from 0.01904\n",
            "Epoch 194/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0242 - acc: 0.9936 - dice_coef: 0.5120 - val_loss: 0.2572 - val_acc: 0.9835 - val_dice_coef: 2.5233e-04\n",
            "\n",
            "Epoch 00194: loss did not improve from 0.01904\n",
            "Epoch 195/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0234 - acc: 0.9942 - dice_coef: 0.5099 - val_loss: 0.2577 - val_acc: 0.9835 - val_dice_coef: 1.4309e-04\n",
            "\n",
            "Epoch 00195: loss did not improve from 0.01904\n",
            "Epoch 196/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0230 - acc: 0.9950 - dice_coef: 0.6053 - val_loss: 0.2629 - val_acc: 0.9835 - val_dice_coef: 2.8527e-06\n",
            "\n",
            "Epoch 00196: loss did not improve from 0.01904\n",
            "Epoch 197/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0244 - acc: 0.9945 - dice_coef: 0.5843 - val_loss: 0.2598 - val_acc: 0.9835 - val_dice_coef: 1.1933e-05\n",
            "\n",
            "Epoch 00197: loss did not improve from 0.01904\n",
            "Epoch 198/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0216 - acc: 0.9959 - dice_coef: 0.6338 - val_loss: 0.2653 - val_acc: 0.9835 - val_dice_coef: 2.8230e-08\n",
            "\n",
            "Epoch 00198: loss did not improve from 0.01904\n",
            "Epoch 199/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0211 - acc: 0.9963 - dice_coef: 0.5154 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 2.1035e-09\n",
            "\n",
            "Epoch 00199: loss did not improve from 0.01904\n",
            "Epoch 200/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0202 - acc: 0.9972 - dice_coef: 0.5174 - val_loss: 0.2658 - val_acc: 0.9835 - val_dice_coef: 4.6312e-09\n",
            "\n",
            "Epoch 00200: loss did not improve from 0.01904\n",
            "Epoch 201/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0198 - acc: 0.9975 - dice_coef: 0.6477 - val_loss: 0.2648 - val_acc: 0.9835 - val_dice_coef: 1.7818e-07\n",
            "\n",
            "Epoch 00201: loss did not improve from 0.01904\n",
            "Epoch 202/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0195 - acc: 0.9976 - dice_coef: 0.6270 - val_loss: 0.2653 - val_acc: 0.9835 - val_dice_coef: 6.3429e-08\n",
            "\n",
            "Epoch 00202: loss did not improve from 0.01904\n",
            "Epoch 203/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0193 - acc: 0.9978 - dice_coef: 0.6546 - val_loss: 0.2648 - val_acc: 0.9835 - val_dice_coef: 2.2873e-07\n",
            "\n",
            "Epoch 00203: loss did not improve from 0.01904\n",
            "Epoch 204/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0192 - acc: 0.9979 - dice_coef: 0.6047 - val_loss: 0.2654 - val_acc: 0.9835 - val_dice_coef: 5.2645e-08\n",
            "\n",
            "Epoch 00204: loss did not improve from 0.01904\n",
            "Epoch 205/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0191 - acc: 0.9981 - dice_coef: 0.6560 - val_loss: 0.2656 - val_acc: 0.9835 - val_dice_coef: 2.5440e-08\n",
            "\n",
            "Epoch 00205: loss did not improve from 0.01904\n",
            "Epoch 206/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0190 - acc: 0.9980 - dice_coef: 0.5553 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 3.1371e-09\n",
            "\n",
            "Epoch 00206: loss improved from 0.01904 to 0.01903, saving model to unet_membrane.hdf5\n",
            "Epoch 207/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0189 - acc: 0.9982 - dice_coef: 0.6086 - val_loss: 0.2655 - val_acc: 0.9835 - val_dice_coef: 3.1705e-08\n",
            "\n",
            "Epoch 00207: loss improved from 0.01903 to 0.01892, saving model to unet_membrane.hdf5\n",
            "Epoch 208/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0188 - acc: 0.9982 - dice_coef: 0.5828 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 3.4667e-09\n",
            "\n",
            "Epoch 00208: loss improved from 0.01892 to 0.01884, saving model to unet_membrane.hdf5\n",
            "Epoch 209/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0189 - acc: 0.9982 - dice_coef: 0.5550 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 2.9340e-09\n",
            "\n",
            "Epoch 00209: loss did not improve from 0.01884\n",
            "Epoch 210/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0189 - acc: 0.9981 - dice_coef: 0.5829 - val_loss: 0.2658 - val_acc: 0.9835 - val_dice_coef: 8.5267e-09\n",
            "\n",
            "Epoch 00210: loss did not improve from 0.01884\n",
            "Epoch 211/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0190 - acc: 0.9980 - dice_coef: 0.5544 - val_loss: 0.2658 - val_acc: 0.9835 - val_dice_coef: 6.5361e-09\n",
            "\n",
            "Epoch 00211: loss did not improve from 0.01884\n",
            "Epoch 212/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0187 - acc: 0.9983 - dice_coef: 0.5832 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 3.0378e-09\n",
            "\n",
            "Epoch 00212: loss improved from 0.01884 to 0.01872, saving model to unet_membrane.hdf5\n",
            "Epoch 213/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0187 - acc: 0.9984 - dice_coef: 0.6638 - val_loss: 0.2652 - val_acc: 0.9835 - val_dice_coef: 1.0358e-07\n",
            "\n",
            "Epoch 00213: loss improved from 0.01872 to 0.01866, saving model to unet_membrane.hdf5\n",
            "Epoch 214/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0186 - acc: 0.9983 - dice_coef: 0.6626 - val_loss: 0.2658 - val_acc: 0.9835 - val_dice_coef: 1.1957e-08\n",
            "\n",
            "Epoch 00214: loss improved from 0.01866 to 0.01859, saving model to unet_membrane.hdf5\n",
            "Epoch 215/300\n",
            "100/100 [==============================] - 3s 26ms/step - loss: 0.0186 - acc: 0.9984 - dice_coef: 0.6370 - val_loss: 0.2658 - val_acc: 0.9835 - val_dice_coef: 6.6083e-09\n",
            "\n",
            "Epoch 00215: loss improved from 0.01859 to 0.01858, saving model to unet_membrane.hdf5\n",
            "Epoch 216/300\n",
            "100/100 [==============================] - 3s 25ms/step - loss: 0.0186 - acc: 0.9984 - dice_coef: 0.6114 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 4.2792e-09\n",
            "\n",
            "Epoch 00216: loss did not improve from 0.01858\n",
            "Epoch 217/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0187 - acc: 0.9982 - dice_coef: 0.6126 - val_loss: 0.2650 - val_acc: 0.9835 - val_dice_coef: 2.3303e-07\n",
            "\n",
            "Epoch 00217: loss did not improve from 0.01858\n",
            "Epoch 218/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0185 - acc: 0.9984 - dice_coef: 0.6152 - val_loss: 0.2655 - val_acc: 0.9835 - val_dice_coef: 3.7849e-08\n",
            "\n",
            "Epoch 00218: loss improved from 0.01858 to 0.01850, saving model to unet_membrane.hdf5\n",
            "Epoch 219/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0186 - acc: 0.9984 - dice_coef: 0.5862 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 1.3045e-09\n",
            "\n",
            "Epoch 00219: loss did not improve from 0.01850\n",
            "Epoch 220/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0184 - acc: 0.9985 - dice_coef: 0.6685 - val_loss: 0.2658 - val_acc: 0.9835 - val_dice_coef: 7.0055e-09\n",
            "\n",
            "Epoch 00220: loss improved from 0.01850 to 0.01844, saving model to unet_membrane.hdf5\n",
            "Epoch 221/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0185 - acc: 0.9984 - dice_coef: 0.5339 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 1.3496e-09\n",
            "\n",
            "Epoch 00221: loss did not improve from 0.01844\n",
            "Epoch 222/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0189 - acc: 0.9981 - dice_coef: 0.4796 - val_loss: 0.2656 - val_acc: 0.9835 - val_dice_coef: 3.2459e-08\n",
            "\n",
            "Epoch 00222: loss did not improve from 0.01844\n",
            "Epoch 223/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0187 - acc: 0.9983 - dice_coef: 0.5594 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 4.5996e-09\n",
            "\n",
            "Epoch 00223: loss did not improve from 0.01844\n",
            "Epoch 224/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0185 - acc: 0.9983 - dice_coef: 0.6414 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 2.9115e-09\n",
            "\n",
            "Epoch 00224: loss did not improve from 0.01844\n",
            "Epoch 225/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0186 - acc: 0.9984 - dice_coef: 0.5589 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 1.5257e-09\n",
            "\n",
            "Epoch 00225: loss did not improve from 0.01844\n",
            "Epoch 226/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0183 - acc: 0.9985 - dice_coef: 0.6137 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 2.3472e-10\n",
            "\n",
            "Epoch 00226: loss improved from 0.01844 to 0.01835, saving model to unet_membrane.hdf5\n",
            "Epoch 227/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0183 - acc: 0.9984 - dice_coef: 0.5052 - val_loss: 0.2658 - val_acc: 0.9835 - val_dice_coef: 1.1528e-08\n",
            "\n",
            "Epoch 00227: loss improved from 0.01835 to 0.01832, saving model to unet_membrane.hdf5\n",
            "Epoch 228/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0184 - acc: 0.9984 - dice_coef: 0.6681 - val_loss: 0.2658 - val_acc: 0.9835 - val_dice_coef: 5.0465e-09\n",
            "\n",
            "Epoch 00228: loss did not improve from 0.01832\n",
            "Epoch 229/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0184 - acc: 0.9984 - dice_coef: nan - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 2.4375e-10\n",
            "\n",
            "Epoch 00229: loss did not improve from 0.01832\n",
            "Epoch 230/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0183 - acc: 0.9985 - dice_coef: 0.5640 - val_loss: 0.2658 - val_acc: 0.9835 - val_dice_coef: 8.4996e-09\n",
            "\n",
            "Epoch 00230: loss improved from 0.01832 to 0.01827, saving model to unet_membrane.hdf5\n",
            "Epoch 231/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0181 - acc: 0.9986 - dice_coef: 0.5923 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 5.2812e-10\n",
            "\n",
            "Epoch 00231: loss improved from 0.01827 to 0.01813, saving model to unet_membrane.hdf5\n",
            "Epoch 232/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0181 - acc: 0.9986 - dice_coef: 0.5918 - val_loss: 0.2658 - val_acc: 0.9835 - val_dice_coef: 6.6896e-09\n",
            "\n",
            "Epoch 00232: loss improved from 0.01813 to 0.01812, saving model to unet_membrane.hdf5\n",
            "Epoch 233/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0180 - acc: 0.9988 - dice_coef: 0.6741 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 2.2028e-09\n",
            "\n",
            "Epoch 00233: loss improved from 0.01812 to 0.01802, saving model to unet_membrane.hdf5\n",
            "Epoch 234/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0181 - acc: 0.9987 - dice_coef: 0.5643 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 6.7708e-11\n",
            "\n",
            "Epoch 00234: loss did not improve from 0.01802\n",
            "Epoch 235/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0182 - acc: 0.9985 - dice_coef: 0.6191 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 4.4597e-09\n",
            "\n",
            "Epoch 00235: loss did not improve from 0.01802\n",
            "Epoch 236/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0182 - acc: 0.9984 - dice_coef: 0.6719 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 2.5594e-09\n",
            "\n",
            "Epoch 00236: loss did not improve from 0.01802\n",
            "Epoch 237/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0180 - acc: 0.9987 - dice_coef: 0.5936 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 2.6000e-09\n",
            "\n",
            "Epoch 00237: loss improved from 0.01802 to 0.01797, saving model to unet_membrane.hdf5\n",
            "Epoch 238/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0180 - acc: 0.9987 - dice_coef: 0.6472 - val_loss: 0.2658 - val_acc: 0.9835 - val_dice_coef: 1.1086e-08\n",
            "\n",
            "Epoch 00238: loss did not improve from 0.01797\n",
            "Epoch 239/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0179 - acc: 0.9987 - dice_coef: 0.6216 - val_loss: 0.2658 - val_acc: 0.9835 - val_dice_coef: 6.5000e-09\n",
            "\n",
            "Epoch 00239: loss improved from 0.01797 to 0.01791, saving model to unet_membrane.hdf5\n",
            "Epoch 240/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0180 - acc: 0.9987 - dice_coef: 0.6195 - val_loss: 0.2657 - val_acc: 0.9835 - val_dice_coef: 2.1482e-08\n",
            "\n",
            "Epoch 00240: loss did not improve from 0.01791\n",
            "Epoch 241/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0181 - acc: 0.9986 - dice_coef: 0.6175 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 1.2458e-09\n",
            "\n",
            "Epoch 00241: loss did not improve from 0.01791\n",
            "Epoch 242/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0180 - acc: 0.9986 - dice_coef: 0.5953 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 1.3632e-09\n",
            "\n",
            "Epoch 00242: loss did not improve from 0.01791\n",
            "Epoch 243/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0179 - acc: 0.9987 - dice_coef: 0.6204 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 1.4625e-09\n",
            "\n",
            "Epoch 00243: loss improved from 0.01791 to 0.01789, saving model to unet_membrane.hdf5\n",
            "Epoch 244/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0180 - acc: 0.9986 - dice_coef: 0.6751 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 4.0444e-09\n",
            "\n",
            "Epoch 00244: loss did not improve from 0.01789\n",
            "Epoch 245/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0180 - acc: 0.9987 - dice_coef: 0.5146 - val_loss: 0.2658 - val_acc: 0.9835 - val_dice_coef: 1.2178e-08\n",
            "\n",
            "Epoch 00245: loss did not improve from 0.01789\n",
            "Epoch 246/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0179 - acc: 0.9987 - dice_coef: 0.6771 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 6.0215e-09\n",
            "\n",
            "Epoch 00246: loss did not improve from 0.01789\n",
            "Epoch 247/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0180 - acc: 0.9986 - dice_coef: 0.6745 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 1.7649e-09\n",
            "\n",
            "Epoch 00247: loss did not improve from 0.01789\n",
            "Epoch 248/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0179 - acc: 0.9987 - dice_coef: 0.6225 - val_loss: 0.2658 - val_acc: 0.9835 - val_dice_coef: 1.0192e-08\n",
            "\n",
            "Epoch 00248: loss did not improve from 0.01789\n",
            "Epoch 249/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0180 - acc: 0.9986 - dice_coef: 0.6751 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 4.4236e-10\n",
            "\n",
            "Epoch 00249: loss did not improve from 0.01789\n",
            "Epoch 250/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0177 - acc: 0.9988 - dice_coef: nan - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 1.2007e-09\n",
            "\n",
            "Epoch 00250: loss improved from 0.01789 to 0.01768, saving model to unet_membrane.hdf5\n",
            "Epoch 251/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0178 - acc: 0.9988 - dice_coef: 0.6788 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 1.3045e-09\n",
            "\n",
            "Epoch 00251: loss did not improve from 0.01768\n",
            "Epoch 252/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0177 - acc: 0.9988 - dice_coef: 0.5690 - val_loss: 0.2658 - val_acc: 0.9835 - val_dice_coef: 7.0732e-09\n",
            "\n",
            "Epoch 00252: loss improved from 0.01768 to 0.01766, saving model to unet_membrane.hdf5\n",
            "Epoch 253/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0177 - acc: 0.9989 - dice_coef: 0.6254 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 9.9305e-10\n",
            "\n",
            "Epoch 00253: loss improved from 0.01766 to 0.01765, saving model to unet_membrane.hdf5\n",
            "Epoch 254/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0176 - acc: 0.9989 - dice_coef: 0.6783 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00254: loss improved from 0.01765 to 0.01763, saving model to unet_membrane.hdf5\n",
            "Epoch 255/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0178 - acc: 0.9987 - dice_coef: 0.6249 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 4.0128e-09\n",
            "\n",
            "Epoch 00255: loss did not improve from 0.01763\n",
            "Epoch 256/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0177 - acc: 0.9988 - dice_coef: 0.6792 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 2.6632e-10\n",
            "\n",
            "Epoch 00256: loss did not improve from 0.01763\n",
            "Epoch 257/300\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 0.0175 - acc: 0.9990 - dice_coef: 0.6274 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 7.9896e-10\n",
            "\n",
            "Epoch 00257: loss improved from 0.01763 to 0.01747, saving model to unet_membrane.hdf5\n",
            "Epoch 258/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0178 - acc: 0.9988 - dice_coef: 0.5989 - val_loss: 0.2646 - val_acc: 0.9835 - val_dice_coef: 3.2319e-04\n",
            "\n",
            "Epoch 00258: loss did not improve from 0.01747\n",
            "Epoch 259/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0178 - acc: 0.9988 - dice_coef: 0.6254 - val_loss: 0.2636 - val_acc: 0.9835 - val_dice_coef: 0.0018\n",
            "\n",
            "Epoch 00259: loss did not improve from 0.01747\n",
            "Epoch 260/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0179 - acc: 0.9986 - dice_coef: 0.5689 - val_loss: 0.2641 - val_acc: 0.9835 - val_dice_coef: 7.5522e-04\n",
            "\n",
            "Epoch 00260: loss did not improve from 0.01747\n",
            "Epoch 261/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0178 - acc: 0.9987 - dice_coef: 0.6223 - val_loss: 0.2648 - val_acc: 0.9835 - val_dice_coef: 9.3361e-06\n",
            "\n",
            "Epoch 00261: loss did not improve from 0.01747\n",
            "Epoch 262/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0179 - acc: 0.9986 - dice_coef: 0.6223 - val_loss: 0.2656 - val_acc: 0.9835 - val_dice_coef: 3.8707e-07\n",
            "\n",
            "Epoch 00262: loss did not improve from 0.01747\n",
            "Epoch 263/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0178 - acc: 0.9986 - dice_coef: 0.5161 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 5.6153e-09\n",
            "\n",
            "Epoch 00263: loss did not improve from 0.01747\n",
            "Epoch 264/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0179 - acc: 0.9985 - dice_coef: 0.5401 - val_loss: 0.2658 - val_acc: 0.9835 - val_dice_coef: 4.7351e-09\n",
            "\n",
            "Epoch 00264: loss did not improve from 0.01747\n",
            "Epoch 265/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0179 - acc: 0.9985 - dice_coef: 0.5155 - val_loss: 0.2659 - val_acc: 0.9835 - val_dice_coef: 1.9274e-09\n",
            "\n",
            "Epoch 00265: loss did not improve from 0.01747\n",
            "Epoch 266/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0178 - acc: 0.9987 - dice_coef: 0.5718 - val_loss: 0.2658 - val_acc: 0.9835 - val_dice_coef: 8.8653e-09\n",
            "\n",
            "Epoch 00266: loss did not improve from 0.01747\n",
            "Epoch 267/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0176 - acc: 0.9987 - dice_coef: 0.6540 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 8.3055e-10\n",
            "\n",
            "Epoch 00267: loss did not improve from 0.01747\n",
            "Epoch 268/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0174 - acc: 0.9989 - dice_coef: nan - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 3.4305e-10\n",
            "\n",
            "Epoch 00268: loss improved from 0.01747 to 0.01742, saving model to unet_membrane.hdf5\n",
            "Epoch 269/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0173 - acc: 0.9990 - dice_coef: 0.6837 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 1.2187e-10\n",
            "\n",
            "Epoch 00269: loss improved from 0.01742 to 0.01733, saving model to unet_membrane.hdf5\n",
            "Epoch 270/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0173 - acc: 0.9990 - dice_coef: 0.6043 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 2.0764e-10\n",
            "\n",
            "Epoch 00270: loss improved from 0.01733 to 0.01728, saving model to unet_membrane.hdf5\n",
            "Epoch 271/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0173 - acc: 0.9990 - dice_coef: 0.6304 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 6.7708e-11\n",
            "\n",
            "Epoch 00271: loss improved from 0.01728 to 0.01726, saving model to unet_membrane.hdf5\n",
            "Epoch 272/300\n",
            "100/100 [==============================] - 2s 25ms/step - loss: 0.0174 - acc: 0.9989 - dice_coef: 0.5746 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 1.0382e-10\n",
            "\n",
            "Epoch 00272: loss did not improve from 0.01726\n",
            "Epoch 273/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0174 - acc: 0.9989 - dice_coef: 0.5737 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 3.6111e-10\n",
            "\n",
            "Epoch 00273: loss did not improve from 0.01726\n",
            "Epoch 274/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0174 - acc: 0.9989 - dice_coef: 0.5483 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 4.5139e-12\n",
            "\n",
            "Epoch 00274: loss did not improve from 0.01726\n",
            "Epoch 275/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0173 - acc: 0.9989 - dice_coef: 0.6838 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 1.3045e-09\n",
            "\n",
            "Epoch 00275: loss did not improve from 0.01726\n",
            "Epoch 276/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0173 - acc: 0.9989 - dice_coef: 0.6857 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 1.3542e-11\n",
            "\n",
            "Epoch 00276: loss did not improve from 0.01726\n",
            "Epoch 277/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0171 - acc: 0.9991 - dice_coef: 0.5772 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 6.2292e-10\n",
            "\n",
            "Epoch 00277: loss improved from 0.01726 to 0.01709, saving model to unet_membrane.hdf5\n",
            "Epoch 278/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0172 - acc: 0.9990 - dice_coef: 0.6863 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 8.1250e-11\n",
            "\n",
            "Epoch 00278: loss did not improve from 0.01709\n",
            "Epoch 279/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0171 - acc: 0.9991 - dice_coef: 0.6894 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 1.0382e-10\n",
            "\n",
            "Epoch 00279: loss improved from 0.01709 to 0.01708, saving model to unet_membrane.hdf5\n",
            "Epoch 280/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0171 - acc: 0.9991 - dice_coef: 0.5777 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 2.2118e-10\n",
            "\n",
            "Epoch 00280: loss did not improve from 0.01708\n",
            "Epoch 281/300\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 0.0171 - acc: 0.9990 - dice_coef: 0.6886 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00281: loss did not improve from 0.01708\n",
            "Epoch 282/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0171 - acc: 0.9990 - dice_coef: 0.6310 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 5.4167e-11\n",
            "\n",
            "Epoch 00282: loss did not improve from 0.01708\n",
            "Epoch 283/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0172 - acc: 0.9990 - dice_coef: 0.6321 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 3.4757e-10\n",
            "\n",
            "Epoch 00283: loss did not improve from 0.01708\n",
            "Epoch 284/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0172 - acc: 0.9990 - dice_coef: 0.5782 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00284: loss did not improve from 0.01708\n",
            "Epoch 285/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0171 - acc: 0.9990 - dice_coef: 0.5506 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 1.2187e-10\n",
            "\n",
            "Epoch 00285: loss did not improve from 0.01708\n",
            "Epoch 286/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0170 - acc: 0.9991 - dice_coef: 0.5799 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 1.8056e-11\n",
            "\n",
            "Epoch 00286: loss improved from 0.01708 to 0.01698, saving model to unet_membrane.hdf5\n",
            "Epoch 287/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0169 - acc: 0.9992 - dice_coef: 0.5804 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00287: loss improved from 0.01698 to 0.01689, saving model to unet_membrane.hdf5\n",
            "Epoch 288/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0169 - acc: 0.9992 - dice_coef: 0.6643 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 1.1285e-10\n",
            "\n",
            "Epoch 00288: loss improved from 0.01689 to 0.01686, saving model to unet_membrane.hdf5\n",
            "Epoch 289/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0169 - acc: 0.9992 - dice_coef: 0.6631 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00289: loss did not improve from 0.01686\n",
            "Epoch 290/300\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 0.0169 - acc: 0.9992 - dice_coef: 0.6919 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 1.3542e-11\n",
            "\n",
            "Epoch 00290: loss did not improve from 0.01686\n",
            "Epoch 291/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0172 - acc: 0.9989 - dice_coef: 0.6324 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00291: loss did not improve from 0.01686\n",
            "Epoch 292/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0169 - acc: 0.9991 - dice_coef: nan - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00292: loss did not improve from 0.01686\n",
            "Epoch 293/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0170 - acc: 0.9990 - dice_coef: 0.6345 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 3.6562e-10\n",
            "\n",
            "Epoch 00293: loss did not improve from 0.01686\n",
            "Epoch 294/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0170 - acc: 0.9990 - dice_coef: 0.6906 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 7.2222e-11\n",
            "\n",
            "Epoch 00294: loss did not improve from 0.01686\n",
            "Epoch 295/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0169 - acc: 0.9991 - dice_coef: 0.6913 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 5.4167e-11\n",
            "\n",
            "Epoch 00295: loss did not improve from 0.01686\n",
            "Epoch 296/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0170 - acc: 0.9990 - dice_coef: 0.5807 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 1.3542e-11\n",
            "\n",
            "Epoch 00296: loss did not improve from 0.01686\n",
            "Epoch 297/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0169 - acc: 0.9990 - dice_coef: 0.6629 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 3.1146e-10\n",
            "\n",
            "Epoch 00297: loss did not improve from 0.01686\n",
            "Epoch 298/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0169 - acc: 0.9991 - dice_coef: 0.6917 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 5.4167e-11\n",
            "\n",
            "Epoch 00298: loss did not improve from 0.01686\n",
            "Epoch 299/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0167 - acc: 0.9992 - dice_coef: 0.6389 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00299: loss improved from 0.01686 to 0.01666, saving model to unet_membrane.hdf5\n",
            "Epoch 300/300\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0167 - acc: 0.9992 - dice_coef: 0.6385 - val_loss: 0.2660 - val_acc: 0.9835 - val_dice_coef: 0.0000e+00\n",
            "\n",
            "Epoch 00300: loss did not improve from 0.01666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdb271205c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "metadata": {
        "id": "UZHxHSp_EB5c",
        "colab_type": "code",
        "outputId": "075490b8-0f5d-4cde-a4c1-01f48817fd97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "model = unet(input_size = (128,128,1))\n",
        "model.load_weights(\"unet_membrane.hdf5\")\n",
        "preds_train = model.predict(X_train,verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:68: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "126/126 [==============================] - 3s 20ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lQAH9rtCNzeP",
        "colab_type": "code",
        "outputId": "d5c76bf2-535c-431f-cf01-3e81fde3469c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "cell_type": "code",
      "source": [
        "ix=12\n",
        "plt.imshow(np.squeeze(preds_train[ix]))\n",
        "plt.show()\n",
        "plt.imshow(np.squeeze(Y_train[ix]),cmap=plt.cm.gray)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nT/QAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nw1CQ1DAUJDUMBUmN/wPcgA1na/et2gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}